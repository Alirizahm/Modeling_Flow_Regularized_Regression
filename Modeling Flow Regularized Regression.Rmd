Instructions
1. Split data: train - validate - test 
2. Draw correlation plot on training data and perform feature selection on
highly correlated features 
3. Fit the models on training data (lambdas = [0.01, 0.1, 1, 10]) 
a. Ridge regression
b. LASSO
4. Choose the best lambda from the validation set
a. Use RMSE as metric
b. Interpret a sample of the coefficients of the best model
i. Ridge regression
ii. LASSO
5. Evaluate the best models on the test data (+ interpretation)
a. MAE
b. MAPE
c. RMSE
```{r}
library(dplyr)
library(caTools)
library(psych)
library(glmnet)
```

```{r}
df= read.csv('boston.csv')
df
```
The Data Used
● For the assignment, we will use the following data:
○ Download link
● The data is about predicting housing price (medv) in
Boston city, features:
○ Criminal rate (crim)
○ Residential land zoned proportion (zn)
○ Non-retail business acres proportion (indus)
○ Is bounds with river (chas)
○ Nitrogen oxides concentration (nox)
○ Number rooms average (rm)
○ Owner age proportion (age)
○ Weighted distance to cities (dis)
○ Accessibility index (rad)
○ Tax rate (tax)
○ Pupil-teacher ratio (ptratio)
○ Black proportion (black)
○ Percent lower status (lstat)

```{r}
# cek type, NA, data dan EDA sederhana
str(df)
summary(df)
View(colSums(is.na(df)))
```
Findings:
1. The average of house price is 22.53
2. The average home owner is 69 years old, the house is likely to be comfortable for parents
3. The maximum value of the crim and zn variables has a maximum value that tends to be high compared to 3rd Qu. There are maybe an outlier on the upper and for the black variable it has a minimum value that tends to be lower compared to 1st Qu there is may be an outlier on the lower
4. There is no missing value dataset


1. Split data: train - validate - test
```{r}
set.seed(123)
# split data rasio 80 :20 
sample <- sample.split(df$medv, SplitRatio = .80)
pre_train <- subset(df, sample == TRUE)
sample_train <- sample.split(pre_train$medv, SplitRatio = .80)
# train-validation data
train <- subset(pre_train, sample_train == TRUE)
validation <- subset(pre_train, sample_train == FALSE)

# test data
test <- subset(df, sample == FALSE)
```

2. Draw correlation plot on training data and perform feature selection on
highly correlated features
```{r}
library(psych)
pairs.panels(train, 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE,  # show density plots
             ellipses = TRUE # show correlation ellipses
) 
# correlated features: rad
# Threshold : > 0.8  

# drop correlated columns
# using the previous results of correlation analysis
drop_cols <- c('rad')

train <- train %>% select(-drop_cols)
validation <-  validation %>% select(-drop_cols)
test <- test %>% select(-drop_cols)
```
Finding:
1. The variable that can be dropped is rad because it correlates more than the threshold of 0.8 with tax, however, tax has a higher value than tax when it is viewed from closest to -1 there is madv

3. Fit models on training data (lambdas = [0.01, 0.1, 1, 10])
```{r}
# feature preprocessing
# to ensure we handle categorical features
x <- model.matrix(medv ~ ., train)[,-1]
y <-  train$medv
# ridge regression
# fit multiple ridge regression with different lambda
# lambda = [0.01, 0.1, 1, 10]
ridge_reg_pointzeroone <- glmnet(x, y, alpha = 0, lambda = 0.01)
print('ridge_reg_pointzeroone')
coef(ridge_reg_pointzeroone)

ridge_reg_pointone <- glmnet(x, y, alpha = 0, lambda = 0.1)
print('ridge_reg_pointone')
coef(ridge_reg_pointone)

ridge_reg_one <- glmnet(x, y, alpha = 0, lambda = 1)
print('ridge_reg_one')
coef(ridge_reg_pointone)

ridge_reg_ten <- glmnet(x, y, alpha = 0, lambda = 10)
print('ridge_reg_ten')
coef(ridge_reg_ten)

# Lasso
Lasso_pointzeroone <- glmnet(x, y, alpha = 1, lambda = 0.01)
print('Lasso_pointzeroone')
coef(ridge_reg_pointzeroone)

Lasso_pointone <- glmnet(x, y, alpha = 1, lambda = 0.1)
print('Lasso_pointone')
coef(ridge_reg_pointone)

Lasso_one <- glmnet(x, y, alpha = 1, lambda = 1)
print('Lasso_one')
coef(ridge_reg_pointone)

Lasso_ten <- glmnet(x, y, alpha = 1, lambda = 10)
print('Lasso_ten')
coef(ridge_reg_ten)
```
Findings: 
1. There is no significant difference when it is compared to ridge with lasso for all lamda values
2. The highest Intercept value is at lamda 10
3. Lamda value of 10 on ridge regression & lasso has the same value
4. Lamda values of 0.1 and 1 have the same value on ridge regression
5. Lamda values 0.1 and 1 have the same value on lasso
6. Interpretation of ridge regression & lasso

Ridge Regresion

"ridge_reg_pointzeroone", medv : 2.80 + -7.97 crim + 3.79 zn + -4.10 indus + 2.89 chas + 1.60 nox + 4.51 rm + 5.67 age + -1.31 dis + -2.42 tax + -9.03 ptratio + 6.57 black + -4.77 lstat

An increase of 1 point in the value of the predictor coefficient, while the value of other features remains, will be associated with an increase in the value of the predictor coefficient on medv

"ridge_reg_pointone"
medv : 2.72 + -7.86 crim + 3.68 zn + -4.20 indus + 2.88 chas + -1.51 nox + 4.52 rm + 5.01 age + -1.26 dis + -4.97 tax + -8.93 ptratio + 6.63 black + -4.70 lstat

An increase of 1 point in the value of the predictor coefficient, while the value of other features remains, will be associated with an increase in the value of the predictor coefficient on medv

"ridge_reg_one"
medv : 2.72 + -7.86 crim + 3.68 zn + -4.20 indus + 2.88 chas + -1.51 nox + 4.52 rm + 5.01 age + -1.26 dis + -4.97 tax + -8.93 ptratio + 6.63 black + -4.70 lstat

An increase of 1 point in the value of the predictor coefficient, while the value of other features remains, will be associated with an increase in the value of the predictor coefficient on medv

"ridge_reg_ten"
medv : 21.7 + -0.06 crim + 0.02 zn + -0.08 indus + 2.10 chas + -4.34 nox + 2.88 rm + -0.008 age + -0.19 dis + -0.003 tax + -0.57 ptratio + 0.005 black + -0.24 lstat

An increase of 1 point in the value of the predictor coefficient, while the value of other features remains, will be associated with an increase in the value of the predictor coefficient on medv

Lasso

"Lasso_pointzeroone"
medv : 2.80 + -7.97 crim + 3.79 zn + -4.10 indus + 2.89 chas + -1.60 nox + 4.51 rm + 5.67 age + -1.31 dis + -2.42 tax + -9.03 ptratio + 6.57 black + -4.77 lstat

An increasing of 1 point in the value of the predictor coefficient, while the value of other features remains, will be associated with an increasing in the value of the predictor coefficient on medv

"Lasso_pointone"
medv : 2.72 + -7.86 crim + 3.68 zn + -4.20 indus + 2.88 chas + -1.51 nox + 4.52 rm + 5.01 age + -1.26 dis + -4.97 tax + -8.93 ptratio + 6.63 black + -4.70 lstat

An increasing of 1 point in the value of the predictor coefficient, while the value of other features remains, will be associated with an increasing in the value of the predictor coefficient on medv

"Lasso_one"
medv : 2.72 + -7.86 crim + 3.68 zn + -4.20 indus + 2.88 chas + -1.51 nox + 4.52 rm + 5.01 age + -1.26 dis + -4.97 tax + -8.93 ptratio + 6.63 black + -4.70 lstat

An increasing of 1 point in the value of the predictor coefficient, while the value of other features remains, will be associated with an increasing in the value of the predictor coefficient on medv

"Lasso_ten"
medv : 21.7 + -0.06 crim + 0.02 zn + -0.08 indus + 2.10 chas + -4.34 nox + 2.88 rm + -0.008 age + -0.19 dis + -0.003 tax + -0.57 ptratio + 0.005 black + -0.24 lstat

An increasing of 1 point in the value of the predictor coefficient, while the value of other features remains, will be associated with an increasing in the value of the predictor coefficient on medv

4. Choose the best lambda from the validation set
```{r}
# Make predictions on the validation data
x_validation <- model.matrix(medv ~., validation)[,-1]
y_validation <- validation$medv

RMSE_ridge_pointzeroone <- sqrt(mean((y_validation - predict(ridge_reg_pointzeroone, x_validation))^2))
paste0("The best lamda of ridge regresion is 0.01 : ", RMSE_ridge_pointzeroone)

RMSE_ridge_pointone <- sqrt(mean((y_validation - predict(ridge_reg_pointone, x_validation))^2))
RMSE_ridge_pointone  

RMSE_ridge_one <- sqrt(mean((y_validation - predict(ridge_reg_one, x_validation))^2))
RMSE_ridge_one 

RMSE_ridge_ten <- sqrt(mean((y_validation - predict(ridge_reg_ten, x_validation))^2))
RMSE_ridge_ten 

# Lasso
RMSE_Lasso_pointzeroone <- sqrt(mean((y_validation - predict(Lasso_pointzeroone, x_validation))^2))
paste0("The best lamda of lasso is 0.01 : ", RMSE_Lasso_pointzeroone)

RMSE_Lasso_pointone <- sqrt(mean((y_validation - predict(Lasso_pointone, x_validation))^2))
RMSE_Lasso_pointone 

RMSE_Lasso_one <- sqrt(mean((y_validation - predict(Lasso_one, x_validation))^2))
RMSE_Lasso_one 

RMSE_Lasso_ten <- sqrt(mean((y_validation - predict(Lasso_ten, x_validation))^2))
RMSE_Lasso_ten 
```
Finding :
1. the best lamda is 0.01 which can be used on the model

5. Evaluate the best models on the test data (+ interpretation)
```{r}
x_test <- model.matrix(medv ~., test)[,-1]
y_test <- test$medv

# Ridge regression
# RMSE
RMSE_ridge_best <- sqrt(mean((y_test - predict(ridge_reg_pointzeroone, x_test))^2))
paste0("Nilai RMSE ridge best : ", RMSE_ridge_best)
# MAE
MAE_ridge_best <- mean(abs(y_test-predict(ridge_reg_pointzeroone, x_test)))
paste0("Nilai MAE ridge best : ", MAE_ridge_best)
# MAPE
MAPE_ridge_best <- mean(abs((predict(ridge_reg_pointzeroone, x_test) - y_test))/y_test)
paste0("Nilai MAPE ridge best : ", MAPE_ridge_best)

# LAsso
# RMSE
RMSE_Lasso_best <- sqrt(mean((y_test - predict(Lasso_pointzeroone, x_test))^2))
paste0("Nilai MAPE Lasso best : ", RMSE_Lasso_best)
# MAE
MAE_Lasso_best <- mean(abs(y_test-predict(Lasso_pointzeroone, x_test)))
paste0("Nilai MAPE Lasso best : ", MAE_Lasso_best)
# MAPE
MAPE_Lasso_best <- mean(abs((predict(Lasso_pointzeroone, x_test) - y_test))/y_test)
paste0("Nilai MAPE Lasso best : ", MAPE_Lasso_best)
```
Findings :
MAE
1. The approximate accuracy value of the model is 6.82
2. The average error distance between the model and the actual data is 3.89 
3. The average percentage of model error with actual data is 17.1%
Lasso
4. The approximate accuracy value of the model is 6.82
5. The average error distance between the model and the actual data is 3.89
6. The average percentage of model error with actual data is 17%
7. Based on the evaluation results are not so different, but lasso tends to be better than ridge regression